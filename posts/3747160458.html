<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>推荐系统论文调研（SIGKDD、SIGIR） | Yangyy's Life</title><meta name="keywords" content="科研实习,推荐系统,论文阅读"><meta name="author" content="yangyy"><meta name="copyright" content="yangyy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="多阅读、多学习">
<meta property="og:type" content="article">
<meta property="og:title" content="推荐系统论文调研（SIGKDD、SIGIR）">
<meta property="og:url" content="https://yangyy.top/posts/3747160458.html">
<meta property="og:site_name" content="Yangyy&#39;s Life">
<meta property="og:description" content="多阅读、多学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s1.ax1x.com/2022/10/27/xhVEB6.jpg">
<meta property="article:published_time" content="2023-02-09T05:39:04.000Z">
<meta property="article:modified_time" content="2023-02-20T08:18:07.000Z">
<meta property="article:author" content="yangyy">
<meta property="article:tag" content="科研实习">
<meta property="article:tag" content="推荐系统">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1.ax1x.com/2022/10/27/xhVEB6.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yangyy.top/posts/3747160458"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '推荐系统论文调研（SIGKDD、SIGIR）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-20 16:18:07'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Yangyy's Life" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/header2.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">36</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/ExcellentBlog/"><i class="fa-fw fa fa-thumbs-up"></i><span> 优秀博客</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fas fa-commenting"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于作者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s1.ax1x.com/2022/10/27/xhVEB6.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Yangyy's Life</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/ExcellentBlog/"><i class="fa-fw fa fa-thumbs-up"></i><span> 优秀博客</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fas fa-commenting"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于作者</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">推荐系统论文调研（SIGKDD、SIGIR）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-02-09T05:39:04.000Z" title="发表于 2023-02-09 13:39:04">2023-02-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-20T08:18:07.000Z" title="更新于 2023-02-20 16:18:07">2023-02-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/">科研</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/posts/3747160458.html" data-flag-title="推荐系统论文调研（SIGKDD、SIGIR）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/3747160458.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/posts/3747160458.html" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="SIGIR-2022"><a class="header-anchor" href="#SIGIR-2022">¶</a>SIGIR 2022</h2>
<h3 id="Interpolative-Distillation-for-Unifying-Biased-and-Debiased-Recommendation"><a class="header-anchor" href="#Interpolative-Distillation-for-Unifying-Biased-and-Debiased-Recommendation">¶</a>Interpolative Distillation for Unifying Biased and Debiased Recommendation</h3>
<p>（用于统一<strong>有偏</strong>和去偏推荐的<strong>插值蒸馏</strong>）</p>
<p>Abstract：大多数推荐系统通过以下任一方式<strong>离线评估模型性能</strong>：1）对事实交互的正常偏差测试；或 2) 使用<strong>随机对照试验</strong>的记录进行去偏试验。事实上，这两个测试都只反映了全貌的一部分：事实交互是<strong>从推荐策略中收集</strong>的，更好地拟合它们意味着有利于平台获得<font color="red">更高的点击率或转化率</font>；相反，<font color="red">去偏测试消除了系统引起的偏差</font>，因此更能反映用户的真实偏好。尽管如此，我们发现现有模型在<strong>两个测试中表现出权衡</strong>，并且缺乏在<strong>两个测试中都表现良好</strong>的方法。 在这项工作中，我们的目标是开发一种双赢的推荐方法，该方法在两个测试中都很强大。这是非常重要的，因为它需要学习一个模型，该模型可以在<font color="red">事实环境（即正常的有偏测试）</font>和<font color="red">反事实环境（即去偏测试）</font>中做出准确的预测。为了实现这一目标，我们通过考虑这两种环境来执行环境感知推荐建模。特别是，我们提出了一个<font color="red">插值蒸馏 (InterD) 框架</font>，它通过蒸馏学生模型在用户-项目对级别对有偏和去偏模型进行插值。我们对三个真实世界的数据集进行了两个测试的实验。实证结果证明了 InterD 的合理性和有效性，它在两个测试中都脱颖而出，特别是在不太受欢迎的项目上表现出显着的进步。</p>
<p><font color="orange">（插值蒸馏+有偏测试+去偏测试）</font></p>
<h3 id="Graph-Trend-Filtering-Networks-for-Recommendation"><a class="header-anchor" href="#Graph-Trend-Filtering-Networks-for-Recommendation">¶</a>Graph Trend Filtering Networks for Recommendation   ***</h3>
<p>（用于推荐的图趋势过滤网络）</p>
<p>Abstract：推荐系统旨在为用户提供个性化服务，在我们的日常生活中发挥着越来越重要的作用。<strong>推荐系统的关键是根据用户的历史在线行为（例如点击、添加到购物车、购买等）来预测用户与项目交互的可能性有多大</strong>。为了利用这些用户-项目交互，越来越多的努力考虑<font color="red">将用户-项目交互作为用户-项目二分图</font>，然后通过图神经网络 (GNN) 在图中执行信息传播。鉴于 GNN 在图形表示学习中的强大功能，这些<strong>基于 GNN 的推荐方法显着提高了推荐性能</strong>。尽管取得了成功，但大多数现有的基于 GNN 的推荐系统都<font color="red">忽略了由不可靠行为（例如，随机/诱饵点击）产生的特征交互并统一处理所有交互</font>，这可能导致次优和不稳定的性能。在本文中，我们<font color="red">研究</font>了现有的基于 GNN 的推荐方法的<font color="red">缺点</font>（例如，非自适应传播和非鲁棒性）。为了解决这些缺点，我们<font color="red">引入了一种有原则的图趋势协同过滤方法，并提出了图趋势过滤网络推荐（GTN）</font>，它可以捕获交互的自适应可靠性。提出了综合实验和消融研究，以验证和理解所提出框架的有效性。我们基于 PyTorch 的实现可用：<a target="_blank" rel="noopener" href="https://github.com/wenqifan03/GTN-SIGIR2022%E3%80%82">https://github.com/wenqifan03/GTN-SIGIR2022。</a></p>
<p><font color="orange">（用户-项目二分图、不可靠行为、CTN）</font></p>
<h3 id="HIEN-Hierarchical-Intention-Embedding-Network-for-Click-Through-Rate-Prediction"><a class="header-anchor" href="#HIEN-Hierarchical-Intention-Embedding-Network-for-Click-Through-Rate-Prediction">¶</a>HIEN: Hierarchical Intention Embedding Network for Click-Through Rate Prediction  ***</h3>
<p>（用于点击率预测的分层意图嵌入网络）</p>
<p>Abstract：点击率 (CTR) 预测在在线广告和推荐系统中起着重要作用，旨在估计用户点击特定项目的概率。<font color="red">特征交互建模和用户兴趣建模方法是CTR预测中的两个热门领域</font>，近年来得到了广泛的研究。然而，这些方法仍然有两个**<font color="red">局限性</font><strong>。首先，传统方法将项目属性视为ID特征，而<font color="red">忽略了属性之间的</font></strong>结构信息<strong>和</strong>关系依赖性**。其次，当从用户-项目交互中挖掘用户兴趣时，当前模型<font color="red">忽略了不同属性的用户意图和项目意图</font>，这<font color="red">缺乏可解释性</font>。基于这一观察，在本文中，我们提出了一种新方法<font color="red">层次意图嵌入网络 (HIEN)</font>，它在构造的属性图中基于自下而上的树聚合考虑属性的依赖关系。HIEN 还<font color="red">捕获</font>用户对不同项目属性的意图以及基于我们提出的分层注意机制的项目<font color="red">意图</font>。在公共和生产数据集上进行的大量实验表明，所提出的模型明显优于最先进的方法。此外，HIEN 可以<font color="red">用作最先进的 CTR 预测方法的输入模块</font>，为这些可能已经在实际系统中大量使用的现有模型带来<font color="red">进一步的性能提升</font>。</p>
<p><font color="orange">（分层(考虑结构信息、关系依赖)、意图(属性代表的意图)）</font></p>
<h3 id="NAS-CTR-Efficient-Neural-Architecture-Search-for-Click-Through-Rate-Prediction"><a class="header-anchor" href="#NAS-CTR-Efficient-Neural-Architecture-Search-for-Click-Through-Rate-Prediction">¶</a>NAS-CTR: Efficient Neural Architecture Search for Click-Through Rate Prediction</h3>
<p>（用于点击率预测的高效神经架构搜索）</p>
<p>Abstract：点击率（CTR）预测已广泛应用于许多机器学习任务，例如在线广告和个性化推荐。不幸的是，给定特定领域的数据集，从<font color="red">巨大的候选空间</font>中搜索有效的特征交互操作和组合需要大量的<font color="red">专家经验和计算成本</font>。最近，神经架构搜索（<font color="red">NAS</font>）在自动<font color="red">发现高质量网络架构</font>方面取得了巨大成功。然而，由于特征交互操作和组合的多样性，现有的基于 NAS 的工作将架构搜索视为离散搜索空间上的<font color="red">黑盒优化问题，效率低下</font>。因此，探索一种更有效的架构搜索方法至关重要。为了实现这个目标，我们提出了NAS-CTR，<font color="red">一种用于 CTR 预测的可区分神经架构搜索方法</font>。首先，我们设计了一个新颖且富有表现力的架构搜索空间和一个连续的松弛方案，使搜索空间可微。其次，我们将用于 CTR 预测的架构搜索制定为对架构具有离散约束的联合优化问题，并利用近端迭代来解决受约束的优化问题。此外，还提出了一种简单而有效的方法来消除跳跃连接的聚集。广泛的实验结果表明，NAS-CTR 在测试准确性和搜索效率方面都优于 SOTA 人工架构和其他基于 NAS 的方法。</p>
<p><font color="orange">（主要是优化寻找有效特征交互的过程，具体方法没看懂）</font></p>
<h3 id="Enhancing-CTR-Prediction-with-Context-Aware-Feature-Representation-Learning"><a class="header-anchor" href="#Enhancing-CTR-Prediction-with-Context-Aware-Feature-Representation-Learning">¶</a>Enhancing CTR Prediction with Context-Aware Feature Representation Learning  ***</h3>
<p>（通过上下文感知的特征表示学习增强CTR预测）</p>
<p>Abstract：CTR预测在现实世界中得到了广泛的应用。许多方法对特征交互进行建模以提高其性能。然而，大多数方法只为每个特征学习固定的表示，而没有考虑每个特征在不同<font color="red">上下文下的不同重要性</font>，导致性能较差。最近，有几种方法尝试<font color="red">学习特征表示的向量级权重</font>，以解决固定表示问题。然而，它们<font color="red">只产生线性变换来改进<strong>固定的特征表示</strong></font>，这些表示仍然不够灵活，无法捕捉不同上下文下每个特征的不同重要性。在本文中，我们提出了一个名为<font color="red">特征细化网络 (FRNet)</font> 的新模块，它在不同上下文中为每个特征<font color="red">学习位级（bit-level）的上下文感知特征表示</font>。FRNet 由两个关键组件组成：1）<font color="red">信息提取单元（IEU）</font>，它捕获上下文信息和交叉特征关系以指导上下文感知特征细化；2）<font color="red">互补选择门（CSGate）</font>，它自适应地将在 IEU 中学习的<font color="red">原始和互补特征表示与比特级权重相结合</font>。值得注意的是，FRNet 与现有的 CTR 方法正交，因此可以应用于许多现有的方法以提高其性能。进行了全面的实验来验证FRNet的有效性、效率和兼容性。</p>
<p><font color="orange">（特征在不同上下文中的重要性，不满足于权重向量，而通过FRNet进行特征的重构，可以直接加在现有的模型上）</font></p>
<h3 id="ESCM2-Entire-Space-Counterfactual-Multi-Task-Model-for-Post-Click-Conversion-Rate-Estimation"><a class="header-anchor" href="#ESCM2-Entire-Space-Counterfactual-Multi-Task-Model-for-Post-Click-Conversion-Rate-Estimation">¶</a>ESCM2: Entire Space  Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation.</h3>
<p>（用于点击后转化率估计的全空间反事实多任务模型）</p>
<p><font color="orange">（完全没看懂摘要，暂时搁置）</font></p>
<h3 id="On-Device-Next-Item-Recommendation-with-Self-Supervised-Knowledge-Distillation"><a class="header-anchor" href="#On-Device-Next-Item-Recommendation-with-Self-Supervised-Knowledge-Distillation">¶</a>On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation</h3>
<p>（具有自监督知识蒸馏的设备上的下一项推荐）</p>
<p>Abstract：基于会话的推荐系统 (SBR) 越来越受欢迎，因为它们可以在<font color="red">不依赖长期用户配置文件</font>的情况下预测用户兴趣，并支持<font color="red">免登录推荐</font>。现代推荐系统以完全基于服务器的方式运行。为了满足<font color="red">数百万用户的需求</font>，需要频繁的模型维护和对并发用户请求的高速处理，这是以巨大的碳足迹为代价的。同时，用户需要将他们的行为<font color="red">数据</font>甚至包括即时环境上下文<font color="red">上传到服务器</font>，引发公众对隐私的关注。设备上的推荐系统通过<font color="red">注重成本的设置和本地推理</font>来规避这两个问题。然而，由于内存和计算资源有限，设备端推荐系统面临着两个基本挑战：(1) <font color="red">如何缩小常规模型</font>的尺寸以适应边缘设备？(2)如何<font color="red">保留原有容量</font>？</p>
<p>以往的研究大多采用张量分解技术以低压缩率压缩常规推荐模型，以避免性能急剧下降。在本文中，我们通过<font color="red">放松张量分解中维度一致性的约束</font>，探索用于下一项推荐的<font color="red">超紧凑模型</font>。<strong>为了补偿压缩造成的容量损失，我们开发了一个自我监督的知识蒸馏框架</strong>，使压缩模型（学生）能够提取原始数据中的基本信息，并通过嵌入改进长尾项目推荐。与原始模型（教师）的重组策略。在两个基准测试中进行的大量实验表明，在尺寸减小 30 倍的情况下，压缩模型几乎没有精度损失，甚至优于其未压缩的对应物。代码发布在https://github.com/xiaxin1998/OD-Rec。</p>
<p><font color="orange">（主要问题就是缩小模型尺寸的情况下尽可能保留原有容量，通过放松约束+自监督的知识蒸馏框架）</font></p>
<h3 id="Single-shot-Embedding-Dimension-Search-in-Recommender-System"><a class="header-anchor" href="#Single-shot-Embedding-Dimension-Search-in-Recommender-System">¶</a>Single-shot Embedding Dimension Search in Recommender System  ***</h3>
<p>（推荐系统中的单次嵌入维度搜索）</p>
<p>Abstract：作为大多数现代<font color="red">深度</font>推荐系统的重要组成部分，<font color="red">特征嵌入</font>将<font color="red">高维稀疏</font>用户/项目特征映射到<font color="red">低维密集</font>嵌入。然而，这些嵌入<font color="red">通常被分配一个统一的维度</font>，存在以下问题：（1）高内存使用和计算成本。(2) 由于次优维度分配导致的次优性能。为了缓解上述问题，一些工作通过将其表述为<font color="red">超参数优化</font>或<font color="red">嵌入剪枝问题</font>来<font color="red">关注自动嵌入维度搜索</font>。但是，它们要么需要为超参数设计良好的搜索空间，要么需要耗时的优化过程。在本文中，我们提出了一种称为 SSEDS 的 Single-Shot Embedding Dimension Search 方法，它可以通过<font color="red">单次嵌入修剪操作</font>有效地<font color="red">为每个特征字段分配维度</font>，同时保持模型的推荐准确性。具体来说，它<font color="red">引入了一个标准</font>来识别每个特征字段的<font color="red">每个嵌入维度的重要性</font>。因此，SSEDS 可以根据相应的维度重要性排名和预定义的参数预算，通过<font color="red">显式减少冗余嵌入维度</font>来自动获得混合维度嵌入。此外，拟议的 SSEDS 与模型无关，这意味着它可以<font color="red">集成到不同的基础推荐模型</font>中。在 CTR（点击率）预测任务的两个广泛使用的公共数据集上进行了广泛的离线实验，结果表明，即使减少了 90% 的参数，SSEDS 仍然可以实现强大的推荐性能。此外，SSEDS还部署在微信订阅平台上，提供实用的推荐服务。为期7天的在线A/B测试结果表明，SSEDS能够显着提升在线推荐模型的性能，同时降低资源消耗。</p>
<p><font color="orange">（通过单次嵌入修剪为不同特征选择合适的维度，从而很大程度减少了模型的参数，引入了一个标准表示模型嵌入维度的重要性）</font></p>
<p><font color="red"><strong>可以发现，基本上对嵌入层的修改都是可以直接加在现有模型上进行优化的，目前已经看到三篇这种类型的文章了。</strong></font></p>
<h3 id="Forest-based-Deep-Recommender"><a class="header-anchor" href="#Forest-based-Deep-Recommender">¶</a>Forest-based Deep Recommender</h3>
<p>（基于森林的深度推荐系统）</p>
<p>随着深度学习技术的发展，深度推荐模型在推荐准确率方面也取得了显着提升。然而，由于实践中<font color="red">候选项目数量众多</font>，<font color="red">偏好计算成本高</font>，这些方法也存在推荐效率低下的问题。最近提出的基于树的深度推荐模型通过<font color="red">在推荐目标的指导下直接学习树结构</font>和表示来缓解这个问题。然而，这样的模型有两个缺点。首先，分层树中的最大堆假设，其中父节点的偏好应该是其子节点偏好中的最大值，在他们的二元分类目标中很难满足。其次，学习索引只包括一棵树。</p>
<p>为此，我们提出了一种基于深度森林的推荐器（简称 DeFoRec）来进行高效推荐。在 DeFoRec 中，保留训练过程中生成的所有树以形成森林。在学习每棵树的节点表示时，我们必须尽可能满足最大堆假设，并在训练阶段模仿树上的波束搜索行为。这是通过 DeFoRec 将训练任务视为同一级别树节点上的多分类来实现的。然而，树节点的数量随着级别呈指数增长，这使得我们可以在 sampled-softmax 技术的指导下训练偏好模型。实验是在真实世界的数据集上进行的，验证了所提出的偏好模型学习方法和树学习方法的有效性。</p>
<h3 id="Explainable-Fairness-in-Recommendation"><a class="header-anchor" href="#Explainable-Fairness-in-Recommendation">¶</a>Explainable Fairness in Recommendation</h3>
<p>（推荐中的可解释公平性）</p>
<p>Abstract：现有关于公平感知推荐的研究主要集中在<font color="red">公平的量化</font>和<font color="red">公平推荐模型</font>的开发上，两者都没有研究一个更实质性的问题——识别<font color="red">推荐中模型差异的根本原因</font>。此信息对于推荐系统设计者了解内在推荐机制并提供有关如何提高决策者模型公平性的见解至关重要。幸运的是，随着 Explainable AI 的快速发展，我们可以使用模型可解释性来深入了解模型（不）公平性。在本文中，我们研究了可解释的公平性问题，这<font color="red">有助于深入了解系统为何公平或不公平</font>，并以更明智和统一的方法指导公平推荐系统的设计。特别，我们关注具有特征感知推荐和<font color="red">暴露不公平</font>的常见设置，但所提出的可解释公平框架是通用的，可以应用于其他推荐设置和公平定义。我们提出了一个称为 CEF 的反事实可解释公平性框架，它生成关于<font color="red">模型公平性的解释</font>，可以在不显着损害性能的情况下提高公平性。CEF框架制定了一个优化问题来学习输入特征的“最小”变化，从而将推荐结果<font color="red">改变到一定程度的公平性</font>。基于每个特征的反事实推荐结果，我们根据fairness-utility trade-off 对所有基于特征的解释进行排序，并选择顶部的作为公平性解释。</p>
<h3 id="User-controllable-Recommendation-Against-font-color-red-Filter-font-Bubble"><a class="header-anchor" href="#User-controllable-Recommendation-Against-font-color-red-Filter-font-Bubble">¶</a>User-controllable Recommendation Against <font color="red">Filter</font> Bubble</h3>
<p>（针对过滤气泡的用户可控推荐）</p>
<p>Abstract：推荐系统通常面临过滤气泡的问题：基于用户特征和历史交互<font color="red">过度推荐同类项目</font>。过滤气泡会沿着反馈循环增长，无意中<font color="red">缩小用户兴趣范围</font>。现有工作通常通过<font color="red">结合准确性以外的目标（例如多样性和公平性</font>）来减轻过滤气泡。然而，它们通常会<font color="red">牺牲准确性</font>，损害模型保真度和用户体验。更糟糕的是，<font color="red">用户</font>不得不<font color="red">被动地接受</font>推荐策略并以具有高延迟的<font color="red">低效方式影响系统</font>，例如，不断提供反馈（例如，喜欢和不喜欢）直到系统识别出用户意图。</p>
<p>这项工作提出了一种称为<font color="red">用户可控推荐系统 </font>(UCRS) 的新推荐器原型，它使<font color="red">用户能够主动控制过滤气泡的缓解</font>。在功能上，1) UCRS 可以在用户深深陷入过滤气泡时<font color="red">提醒</font>用户。2) UCRS 支持<font color="red">四种控制命令</font>供用户在不同粒度上<font color="red">缓解气泡</font>。3) UCRS 可以响应控制并即时调整建议。<font color="red">调整的关键在于阻止过时的用户表征对推荐的影响</font>，其中包含与控制命令不一致的历史信息。因此，我们开发了一个<font color="red">因果关系增强的用户可控推理 (UCI) 框架</font>，它可以在推理阶段根据用户控制快速修改推荐，并利用反事实推理来减轻过时用户表示的影响。在三个数据集上的实验验证了 UCI 框架可以根据用户控制有效地推荐更多想要的项目，在<font color="red"><strong>准确性和多样性</strong></font>方面都表现出了良好的性能。</p>
<p><font color="orange">（过滤气泡问题、多样性牺牲公平性、用户被动低效反馈，提出<strong>用户可控</strong>的推荐系统、修改历史表征的影响）</font></p>
<h3 id="Unify-Local-and-Global-Information-for-Top-N-Recommendation"><a class="header-anchor" href="#Unify-Local-and-Global-Information-for-Top-N-Recommendation">¶</a>Unify Local and Global Information for Top-N Recommendation</h3>
<p>（统一本地和全局信息用于Top-N推荐）</p>
<p>Abstract：<font color="red">知识图谱 (KG)</font> 集成了复杂的信息并包含丰富的语义，被广泛认为是增强推荐系统的辅助信息。然而，大多数现有的基于知识图谱的方法都<font color="red">集中在对图中的结构信息进行编码</font>，而<font color="red">没有利用用户-项目交互数据中的协作信号</font>，这对于理解用户偏好很重要。因此，这些模型学习到的表示不足以表示推荐环境中用户和项目的语义信息。<font color="red">两种数据的结合</font>为解决这个问题提供了一个很好的机会，但它面临以下挑战：i）用户-项目交互数据中的内在关联很难从用户或项目的一侧捕获；ii) 捕获<font color="red">整个 KG 上的知识关联会引入噪声</font>并对推荐结果产生不同程度的影响；iii) 两种数据之间的<font color="red">语义鸿沟</font>难以消除。</p>
<p>为了解决这一研究差距，我们提出了一种名为 KADM 的新型二重奏表示学习框架，<font color="red">以融合局部信息（用户-项目交互数据）和全局信息（外部知识图）以进行前 N 推荐</font>，它由<font color="red">两个独立的子模型</font>组成楷模。一种通过使用知识感知共同注意机制发现局部信息中的内部相关性来学习局部表示，另一种通过使用关系感知注意网络对全局信息中的知识关联进行编码来学习全局表示。这两个子模型作为语义融合网络的一部分被<font color="red">联合训练</font>以计算用户偏好，这区分了两个子模型在特殊上下文下的贡献。我们对两个真实世界的数据集进行实验，评估表明，KADM 明显优于最先进的方法。进一步的消融研究证实，二重架构在推荐任务上的<font color="red">表现明显优于任一子模型</font>。</p>
<p><font color="orange">（目前专注于知识图谱的全局信息学习，但是缺乏对用户项目特征交互的关注，目前来看这两者有一定冲突、会互相影响，提出一个双重架构的模型，两个独立的子模型联合训练）</font></p>
<h3 id="Less-is-More-Reweighting-Important-Spectral-Graph-Features-for-Recommendation"><a class="header-anchor" href="#Less-is-More-Reweighting-Important-Spectral-Graph-Features-for-Recommendation">¶</a>Less is More: Reweighting Important Spectral Graph Features for Recommendation</h3>
<p>(为推荐重新赋权重要的光谱图特征)</p>
<p>Abstract：尽管图卷积网络 (GCN) 在推荐系统和协同过滤 (CF) 方面取得了巨大的成功，但它们，尤其是<font color="red">核心组件</font>（\textiti.e.，<font color="red">邻域聚合</font>）如何促进推荐的机制并没用被理想地研究。为了揭示 GCN 在推荐方面的有效性，我们首先从谱的角度对其进行分析，并发现了两个重要发现：（1）只有一小部分强调邻域平滑度和差异的谱图特征有助于推荐的准确性，而<font color="red">大多数图信息可以被视为甚至降低性能的噪声，</font>并且（2）邻域聚合的重复强调平滑的特征并以无效的方式滤除噪声信息。基于以上两个发现，我们提出了一种新的 GCN 学习方案，通过用简单而有效的<font color="red">图去噪编码器 (GDE) 代替邻域聚合来进行推荐</font>，它充当带通滤波器来捕获重要的图特征。我们表明，我们提出的方法减轻了过度平滑，并且与可以考虑任意跳邻域的无限层 GCN 相当。最后，我们在不引入额外复杂性的情况下动态调整负样本的梯度以加快模型训练。对五个真实世界数据集的广泛实验表明，我们提出的方法不仅优于最先进的技术，而且比 LightGCN 实现了 12 倍的加速。</p>
<h3 id="A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation"><a class="header-anchor" href="#A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation">¶</a>A Review-aware Graph Contrastive Learning Framework for Recommendation</h3>
<p>（一个用于推荐的评论感知图对比学习框架）</p>
<p>Abstract：大多数现代推荐系统通过两个部分预测用户的偏好：<font color="red">用户和项目嵌入学习</font>，然后是用户-项目<font color="red">交互建模</font>。通过利用伴随<font color="red">用户评分的辅助评论信息</font>，许多现有的基于评论的推荐模型<font color="red">通过</font>历史评论或在可用的用户-项目目标<font color="red">评论</font>的帮助下<font color="red">更好地建模</font>用户-项目交互来丰富用户/项目嵌入学习能力。尽管取得了重大进展，但我们认为当前基于评论的推荐解决方案存在两个缺点。首先，由于基于评论的推荐<font color="red">可以自然地形成一个用户-项目二分图</font>，具有来自相应用户-项目评论的边缘特征，如何更好地利用这种独特的图结构进行推荐？第二，虽然当前大多数模型的用户行为有限，但我们能否利用<font color="red">评论感知图中独特的自我监督信号</font>来更好地指导两个推荐组件？为此，在本文中，我们提出了一种新颖的<font color="red">评论感知图对比学习 (RGCL) 框架</font>，用于基于评论的推荐。具体来说，我们首先构建一个具有<font color="red">来自评论的特征增强边的评论感知用户-项目图</font>，其中每个边缘特征由<font color="red">用户-项目评分和相应的评论语义组成</font>。这个带有<font color="red">特征增强边的图</font>可以帮助仔细学习每个邻居节点的权重，用于用户和项目表示学习。之后，我们设计了两个额外的对比学习任务（即 Node Discrimination and Edge Discrimination）为推荐过程中的两个组件提供自监督信号。最后，对五个基准数据集的广泛实验证明了我们提出的 RGCL 与最先进的基线相比的优越性。</p>
<p><font color="orange">（用户-项目二分图+评论感知的特征增强边+对比学习）</font></p>
<h3 id="Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation"><a class="header-anchor" href="#Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation">¶</a>Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation</h3>
<p>（图扩充是否必要？：用于推荐的简单图对比学习）</p>
<p>Abstract：对比学习 (CL) 最近在推荐领域激发了一系列富有成果的研究，因为它<font color="red">从原始数据中提取自我监督信号的能力</font>与推荐系统解决<font color="red">数据稀疏性问题的需求</font>完全一致。基于 CL 的推荐模型的典型流程是<font color="red">首先使用结构扰动扩充用户-项目二部图</font>，然后最大化不同图扩充之间的节点表示一致性。尽管这种范式被证明是有效的，但性能提升的基础仍然是个谜。在本文中，我们首先通过实验揭示，在基于 CL 的推荐模型中，<font color="red">CL 通过学习更统一的用户/项目表示来运作</font>，这可以隐含地减轻流行偏差。同时，我们揭示了<font color="red">图形扩充</font>，曾经被认为是必要的，<font color="red">只是起到了微不足道的作用</font>。基于这一发现，我们提出了一种<font color="red">简单的 CL 方法</font>，该方法丢弃图形扩充，而是<font color="red">向嵌入空间添加均匀噪声以创建对比视图</font>。对三个基准数据集的综合实验研究表明，虽然它看起来非常简单，但所提出的方法可以平滑地调整学习表示的均匀性，并且在推荐准确性和训练效率方面比基于图增强的方法具有明显的优势。代码发布在 <a target="_blank" rel="noopener" href="https://github.com/Coder-Yu/QRec%E3%80%82">https://github.com/Coder-Yu/QRec。</a></p>
<p><font color="orange">（对比学习通常被认为和图扩充相关，但是实际并不相关，因此丢弃图形扩充）</font></p>
<h3 id="AutoLossGen-Automatic-Loss-Function-Generation-for-Recommender-Systems"><a class="header-anchor" href="#AutoLossGen-Automatic-Loss-Function-Generation-for-Recommender-Systems">¶</a>AutoLossGen: Automatic Loss Function Generation for Recommender Systems</h3>
<p>（推荐系统的自动损失函数生成）</p>
<p>Abstract：在推荐系统中，损失函数的选择至关重要，因为良好的损失可能会显着提高模型性能。然而，由于问题的复杂性，<font color="red">手动设计一个好的损失是一个很大的挑战</font>。以前的大部分工作都集中在手工制作的损失函数上，这需要大量的专业知识和人力。在本文中，受最近<font color="red">自动化机器学习发展</font>的启发，我们提出了一种<font color="red">自动损失函数生成框架 AutoLossGen</font>，它能够直接从基本数学运算符构建生成损失函数，而<font color="red">无需先验损失结构知识</font>。更具体地说，我们开发了一个由<font color="red">强化学习驱动</font>的控制器模型来生成损失函数，并<font color="red">制定迭代和交替优化计划</font>来更新控制器模型和推荐模型的参数。推荐系统中自动损失生成的一个挑战是推荐<font color="red">数据集的极度稀疏性</font>，这导致了损失生成和搜索的稀疏奖励问题。为了解决这个问题，我们进一步开发了一种<font color="red">奖励过滤机制</font>，以有效地产生损失。实验结果表明，我们的框架设法为不同的推荐模型和数据集创建定制的损失函数，并且生成的损失比常用的基线损失提供更好的推荐性能。此外，<font color="red">大部分产生的损失是可转移</font>的，即基于一个模型和数据集产生的损失也适用于另一个模型或数据集。</p>
<h1>对比学习专题</h1>
<p>（感觉数据/特征增强似乎是其中一个很常见的技术）</p>
<h2 id="SIGIR2022"><a class="header-anchor" href="#SIGIR2022">¶</a>SIGIR2022</h2>
<h3 id="Hypergraph-Contrastive-Collaborative-Filtering"><a class="header-anchor" href="#Hypergraph-Contrastive-Collaborative-Filtering">¶</a>Hypergraph Contrastive Collaborative Filtering</h3>
<p>（超图对比协同过滤）</p>
<p>Abstract：<font color="red">协同过滤 (CF) </font>已成为将用户和项目参数化到潜在表示空间的基本范例，以及来自交互数据的相关模式。在各种 CF 技术中，基于 GNN 的推荐系统（例如 PinSage 和 LightGCN）的开发提供了最先进的性能。然而，现有解决方案尚未很好地探索两个关键挑战：i）更深层次的基于图的 CF 架构的<font color="red">过度平滑效应</font>可能导致无法区分的用户表示和推荐结果的退化。ii) 监督信号（即用户-项目交互）在现实中通常稀缺且分布不均，这限制了 CF 范式的表示能力。为了应对这些挑战，<font color="red">我们提出了一种新的自监督推荐框架超图对比协同过滤（HCCF）</font>，以<font color="red"><strong>通过超图增强的跨视图对比学习架构共同捕获本地和全球协作关系</strong></font>。特别是，设计的超图结构学习增强了基于 GNN 的 CF 范式的辨别能力，<font color="red">全面捕获用户之间复杂的高阶依赖关系</font>。此外，我们的 HCCF 模型有效地将超图结构编码与自我监督学习相结合，以基于超图自判别来增强推荐系统的表示质量。在三个基准数据集上进行的大量实验证明了我们的模型优于各种最先进的推荐方法，以及针对稀疏用户交互数据的鲁棒性。实现代码可在 <a target="_blank" rel="noopener" href="https://github.com/akaxlh/HCCF">https://github.com/akaxlh/HCCF</a> 获得。</p>
<h3 id="A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation-v2"><a class="header-anchor" href="#A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation-v2">¶</a>A Review-aware Graph Contrastive Learning Framework for Recommendation</h3>
<p>（一个用于推荐的评论感知图对比学习框架）</p>
<p>Abstract：大多数现代推荐系统通过两个部分预测用户的偏好：<font color="red">用户和项目嵌入学习</font>，然后是<font color="red">用户-项目交互建模</font>。通过利用伴随用户评分的<font color="red">辅助评论信息</font>，许多现有的基于评论的推荐模型通过历史评论来<font color="red">丰富用户/项目嵌入学习能力</font>或在可用的用户-项目目标评论的帮助下<font color="red">更好地建模用户-项目交互</font>。尽管取得了重大进展，但我们认为当前基于评论的推荐解决方案存在两个缺点。首先，由于基于评论的推荐<font color="red">可以自然地形成一个用户-项目二分图</font>，具有来自相应<font color="red">用户-项目评论的边缘特征（edge features）</font>，如何更好地利用这种独特的图结构进行推荐？第二，虽然当前大多数模型的用户行为有限，但我们能否利用<font color="red">评论感知图中独特的自我监督信号来更好地指导两个推荐组件</font>？为此，在本文中，我们提出了一种新颖的<font color="red">评论感知图对比学习 (RGCL) 框架</font>，用于基于评论的推荐。具体来说，我们<font color="red">首先构建一个</font>具有来自评论的特征增强边的<font color="red">评论感知用户-项目图</font>，其中每个边缘特征由用户-项目评分和相应的评论语义组成。这个带有特征增强边的图可以帮助仔细学习每个邻居节点的权重，用于用户和项目表示学习。之后，我们设计了两个额外的对比学习任务（即 Node Discrimination and Edge Discrimination）为推荐过程中的两个组件提供自监督信号。最后，对五个基准数据集的广泛实验证明了我们提出的 RGCL 与最先进的基线相比的优越性。</p>
<h3 id="Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation-v2"><a class="header-anchor" href="#Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation-v2">¶</a>Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation</h3>
<p>（图扩充是否必要？：用于推荐的简单图对比学习）</p>
<p>Abstract：对比学习 (CL) 最近在推荐领域激发了一系列富有成果的研究，因为<font color="red">它从原始数据中提取自我监督信号的能力与推荐系统解决数据稀疏性问题的需求完全一致。</font>基于 CL 的推荐模型的典型流程是<font color="red">首先使用结构扰动扩充用户-项目二部图</font>，然后<font color="red">最大化不同图扩充之间的节点表示一致性</font>。尽管这种范式被证明是有效的，但性能提升的基础仍然是个谜。在本文中，我们首先通过实验揭示，在基于 CL 的推荐模型中，<font color="red">CL 通过学习更统一的用户/项目表示来运作，这可以隐含地减轻流行偏差</font>。同时，我们揭示了<font color="red">图形扩充</font>，曾经被认为是必要的，<font color="red">只是起到了微不足道的作用</font>。基于这一发现，我们提出了一种简单的 CL 方法，该方法<font color="red">丢弃图形扩充</font>，而是向嵌入空间添加均匀噪声以创建对比视图。对三个基准数据集的综合实验研究表明，虽然它看起来非常简单，但所提出的方法可以平滑地调整学习表示的均匀性，并且在推荐准确性和训练效率方面比基于图增强的方法具有明显的优势。代码发布在 <a target="_blank" rel="noopener" href="https://github.com/Coder-Yu/QRec%E3%80%82">https://github.com/Coder-Yu/QRec。</a></p>
<h3 id="Multi-level-Cross-view-Contrastive-Learning-for-Knowledge-aware-Recommender-System"><a class="header-anchor" href="#Multi-level-Cross-view-Contrastive-Learning-for-Knowledge-aware-Recommender-System">¶</a>Multi-level Cross-view Contrastive Learning for Knowledge-aware Recommender System</h3>
<p>（知识感知推荐系统的多层交叉试图对比学习）</p>
<p>Abstract：<font color="red">知识图谱（KG）</font>在推荐系统中扮演着越来越重要的角色。最近，基于图神经网络（GNNs）的模型逐渐成为知识感知推荐（KGR）的主题。然而，基于 GNN 的 KGR 模型<font color="red">存在一个天然的缺陷，即稀疏监督信号问题</font>，这可能使其实际性能有所下降。受近期<font color="red">对比学习在从数据本身挖掘监督信号方面取得的成功</font>的启发，在本文中，我们专注于<font color="red">探索 KG 感知推荐中的对比学习</font>，并提出了一种新的<font color="red">多层次跨视图对比学习机制</font>，称为 MCCLK。与传统的对比学习方法不同，<font color="red">传统的对比学习方法</font>通过统一的数据增强方案（如损坏或丢弃）生成两个图视图，我们综合考虑了 KG 感知推荐的三种不同图视图，<font color="red">包括全局级结构视图、局部级协作和语义视图</font>。具体来说，我们将用户-项目图视为协作视图，将项目-实体图视为语义视图，将用户-项目-实体图视为结构视图。因此，<font color="red">MCCLK 在本地和全局级别的三个视图中执行对比学习，以自我监督的方式挖掘综合图形特征和结构信息。</font>此外，在语义视图中，提出了一种k-最近邻（k NN）项-项语义图构建模块，以捕获先前工作通常忽略的重要项-项语义关系。在三个基准数据集上进行的大量实验表明，我们提出的方法优于现有技术。这些实现可在以下位置获得：<a target="_blank" rel="noopener" href="https://github.com/CCIIPLab/MCCLK%E3%80%82">https://github.com/CCIIPLab/MCCLK。</a></p>
<h3 id="Knowledge-Graph-Contrastive-Learning-for-Recommendation"><a class="header-anchor" href="#Knowledge-Graph-Contrastive-Learning-for-Recommendation">¶</a>Knowledge Graph Contrastive Learning for Recommendation</h3>
<p>（用于推荐的知识图对比学习）</p>
<p>Abstract：知识图 (KG) 已被用作有用的辅助信息来提高推荐质量。在这些推荐系统中，知识图谱信息通常<font color="red">包含丰富的事实</font>和<font color="red">项目之间固有的语义相关性</font>。然而，此类方法的成功依赖于高质量的知识图谱，并且可能无法学习具有两个挑战的质量表示：i）<font color="red">实体的长尾分布导致 KG 增强项目表示的<strong>监督信号稀疏</strong></font>；ii) 真实世界的知识图通常是嘈杂的，并且包含项目和实体之间<font color="red">与主题无关的连接</font>。这种 KG 稀疏性和噪声使得项目-实体依赖关系偏离反映其真实特征，这显著放大了噪声效应并阻碍了用户偏好的准确表示。</p>
<p>为了填补这一研究空白，我们设计了一个<font color="red">通用的知识图谱对比学习框架 (KGCL)</font>，它可以减轻知识图谱增强推荐系统的信息噪声。具体来说，我们提出了一种<font color="red">知识图增强模式来抑制信息聚合中的 KG 噪声</font>，并为项目推导出更稳健的知识感知表示。此外，我们利用来自 KG 增强过程的额外监督信号来指导交叉视图对比学习范式，在梯度下降中为无偏见的用户-项目交互提供更大的作用，并进一步抑制噪声。在三个公共数据集上进行的广泛实验证明了我们的 KGCL 优于最先进技术的一贯优势。KGCL 还在具有稀疏用户-项目交互的推荐场景中实现了强大的性能，长尾和嘈杂的 KG 实体。我们的实现代码可在 <a target="_blank" rel="noopener" href="https://github.com/yuh-yang/KGCL-SIGIR22">https://github.com/yuh-yang/KGCL-SIGIR22</a> 获得。</p>
<h2 id="KDD2022"><a class="header-anchor" href="#KDD2022">¶</a>KDD2022</h2>
<h3 id="CrossCBR-Cross-view-Contrastive-Learning-for-Bundle-Recommendation"><a class="header-anchor" href="#CrossCBR-Cross-view-Contrastive-Learning-for-Bundle-Recommendation">¶</a>CrossCBR: Cross-view Contrastive Learning for Bundle Recommendation</h3>
<p>（用于捆绑推荐的跨视图对比学习）</p>
<p>Abstract：捆绑推荐旨在向用户推荐一组相关的物品，能够<font color="red">一站式地方便地满足用户的各种需求</font>。最近的方法通常利用<font color="red">用户-捆绑</font>和<font color="red">用户-项目交互信息</font>来获得用户和捆绑的信息表示，分别对应于捆绑视图和项目视图。然而，他们要么使用没有差异化的统一视图，要么松散地结合两个独立视图的预测，而<font color="red">忽略了两个视图表示之间至关重要的协作关联。</font></p>
<p>在这项工作中，我们建议<font color="red">通过跨视图对比学习来模拟两种不同视图之间的合作关联</font>。通过鼓励两个单独学习的视图对齐，每个视图都可以从另一个视图中提取互补信息，<font color="red">从而实现相互增强</font>。此外，通过扩大不同用户/捆绑的分散度，增强了表示的自辨别能力。在三个公共数据集上进行的大量实验表明，我们的方法大大优于 SOTA 基线。同时，我们的方法需要最少的三组嵌入（用户、捆绑和项目）参数，并且由于更简洁的图结构和图学习模块，计算成本大大降低。此外，各种消融和模型研究揭开了工作机制的神秘面纱并证明了我们的假设。代码和数据集可在 <a target="_blank" rel="noopener" href="https://github.com/mysbupt/CrossCBR">https://github.com/mysbupt/CrossCBR</a> 获得。</p>
<h3 id="COSTA-Covariance-Preserving-Feature-Augmentation-for-Graph-Contrastive-Learning"><a class="header-anchor" href="#COSTA-Covariance-Preserving-Feature-Augmentation-for-Graph-Contrastive-Learning">¶</a>COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning</h3>
<p>（图对比学习的协方差保持特征增强）</p>
<p>Abstract：<font color="red">图对比学习 (GCL) </font>改进了图表示学习，导致各种下游任务的 SOTA。<font color="red">图扩充步骤是 GCL 的一个重要但很少被研究的步骤</font>。在本文中，我们表明通过<font color="red">图扩充获得的节点嵌入是高度有偏的</font>，在一定程度上限制了对比模型从学习下游任务的判别特征。因此，我们不是研究输入空间中的图扩充，而是建议对隐藏的特征增强。受所谓的矩阵草图的启发，我们提出了 COSTA，这是一种用于 GCL 的新型协方差保留<font color="red">特征空间增强框架</font>，它通过维护原始特征的“良好草图”来<font color="red">生成增强特征</font>。为了突出使用 COSTA 进行特征增强的优越性，我们研究了一种节省内存和计算的单视图设置（除了多视图设置）。我们表明，使用 COSTA 进行的特征扩充与基于图扩充的模型相比取得了可比/更好的结果。</p>
<h3 id="Contrastive-Cross-domain-Recommendation-in-Matching"><a class="header-anchor" href="#Contrastive-Cross-domain-Recommendation-in-Matching">¶</a>Contrastive Cross-domain Recommendation in Matching</h3>
<p>（匹配中的对比跨域推荐）</p>
<p>Abstract：<font color="red">跨域推荐（CDR）旨在借助源域在目标域中提供更好的推荐结果</font>，在现实系统中被广泛使用和探索。然而，<font color="red">匹配（即候选生成）模块中的 CDR 在表示学习和知识迁移中都与<strong>数据稀疏性</strong>和<strong>流行偏差</strong>问题作斗争</font>。在这项工作中，我们提出了一种用于 CDR 匹配的新型对比跨域推荐 (CCDR) 框架。具体来说，我们构建了一个巨大的多元化偏好网络来捕获反映用户不同兴趣的多种信息，并设计了<font color="red">一个域内对比学习（intra-CL）</font>和<font color="red">三个域间对比学习（inter-CL）</font>任务以更好地表示学习和知识转移。intra-CL 通过图形增强在目标域内实现更有效和平衡的训练，而 inter-CL 从用户、分类和邻居方面构建不同类型的跨域交互。在实验中，CCDR 在现实世界系统中的离线和在线评估上取得了显着改进。目前，我们已经在微信头条上部署了CCDR，影响了很多用户。源代码在 <a target="_blank" rel="noopener" href="https://github.com/lqfarmer/CCDR%E3%80%82">https://github.com/lqfarmer/CCDR。</a></p>
<h2 id="WSDM2022"><a class="header-anchor" href="#WSDM2022">¶</a>WSDM2022</h2>
<h3 id="Contrastive-Learning-for-Representation-Degeneration-Problem-in-Sequential-Recommendation"><a class="header-anchor" href="#Contrastive-Learning-for-Representation-Degeneration-Problem-in-Sequential-Recommendation">¶</a>Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation</h3>
<p>（顺序推荐中表征退化问题的对比学习）</p>
<p>Abstract：<font color="red">Transformer 和 BERT</font> 等顺序深度学习模型的最新进展极大地促进了顺序推荐。然而，根据我们的研究，这些模型生成的<font color="red">项目嵌入的分布倾向于退化为各向异性形状</font>，<font color="red">这可能导致嵌入之间的语义相似度很高</font>。在本文中，首先提供了这种表示<font color="red">退化问题的实证和理论研究</font>，在此基础上提出了一种新的<font color="red">推荐模型 DuoRec </font>来改善项目嵌入分布。具体来说，鉴于对比学习的均匀性，为 DuoRec 设计了<font color="red">对比正则化以重塑序列表示的分布</font>。鉴于推荐任务是<font color="red">通过点积测量同一空间中序列表示和项目嵌入之间的相似性</font>来执行的约定，正则化可以隐式应用于项目嵌入分布。<font color="red">现有的对比学习方法主要依赖于通过项目裁剪、屏蔽或重新排序对用户-项目交互序列进行数据级增强</font>，并且很难提供语义一致的增强样本。在 DuoRec 中，<font color="red">提出了一种基于 Dropout 的模型级增强，以实现更好的语义保</font><font color="red">留</font>。此外，还开发了一种新的抽样策略，其中具有相同目标项的序列被选为硬阳性样本。对五个数据集进行的大量实验表明，与基线方法相比，所提出的 DuoRec 模型具有优越的性能。学习表示的可视化结果证实 DuoRec 可以在很大程度上缓解表示退化问题。</p>
<h3 id="Contrastive-Meta-Learning-with-Behavior-Multiplicity-for-Recommendation"><a class="header-anchor" href="#Contrastive-Meta-Learning-with-Behavior-Multiplicity-for-Recommendation">¶</a>Contrastive Meta Learning with Behavior Multiplicity for Recommendation</h3>
<p>Abstract：一个消息灵通的推荐框架不仅可以<font color="red">帮助用户识别他们感兴趣的项目</font>，而且有利于各种在线平台（例如电子商务、社交媒体）的收入。传统的推荐模型<font color="red">通常假设用户和物品之间只存在单一类型的交互</font>，而<font color="red">无法</font>从多类型的用户行为数据（例如页面浏览、添加到收藏夹和购买）中<font color="red">对多重用户-物品关系进行建模</font>。虽然最近的一些研究建议捕获不同类型行为之间的依赖关系，但很少探索两个重要的挑战：i) <font color="red">处理目标行为（例如，购买）下的稀疏监督信号</font>。ii) 通过定制的依赖建模来捕捉个性化的多行为模式。为了应对上述挑战，我们设计了一个新的模型 CML，<font color="red">对比元学习 (CML)</font>，为不同用户维护专用的跨类型行为依赖性。特别是，我们<font color="red">提出了一个多行为对比学习框架</font>，<font color="red">通过构建的对比损失来提炼跨不同类型行为的可迁移知识</font>。此外，为了捕捉多样化的多行为模式，我们设计了一个对比元网络来为不同用户编码定制的行为异质性。对三个真实世界数据集的广泛实验表明，我们的方法始终优于各种最先进的推荐方法。我们的实证研究进一步表明，对比元学习范式为捕捉推荐中的行为多样性提供了巨大的潜力。我们在以下位置发布了我们的模型实现：<a target="_blank" rel="noopener" href="https://github.com/weiwei1206/CML.git%E3%80%82">https://github.com/weiwei1206/CML.git。</a></p>
<h3 id="Bringing-Your-Own-View-Graph-Contrastive-Learning-without-Prefabricated-Data-Augmentations"><a class="header-anchor" href="#Bringing-Your-Own-View-Graph-Contrastive-Learning-without-Prefabricated-Data-Augmentations">¶</a>Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations</h3>
<p>（提出自己的观点：无需预制数据增强的图形对比学习）</p>
<p>Abstract：自我监督最近在其图形学习的新领域蓬勃发展。它促进了对下游任务有益的图形表示；但它的成功可能<font color="red">取决于手工艺领域的知识或通常代价高昂的反复试验</font>。即使是<font color="red">它最先进的代表，图对比学习 (GraphCL)</font>，也不能完全摆脱这些需求，因为 GraphCL 使用预制的先验，这反映在图数据增强的临时手动选择上。我们的工作旨在通过回答以下问题来<font color="red">推进 GraphCL</font>：如何表示图增强视图的空间？可以依靠什么原则来学习该空间的先验知识？可以构建什么框架来学习先验与对比学习？因此，我们扩展了增强集中的预制离散先验，到图形生成器参数空间中的可学习连续先验，假设图形先验本身，类似于图像流形的概念，可以通过数据生成来学习。此外，为了形成对比视图而不因先验可学习性而崩溃为琐碎的解决方案，我们利用了信息最小化 (InfoMin) 和信息瓶颈 (InfoBN) 的原则来规范学习的先验。最终，对比学习、InfoMin和InfoBN有机地融入到一个双层优化框架中。我们的原则性和自动化方法已被证明在小图基准上与最先进的图自监督方法（包括 GraphCL）相比具有竞争力；并在大规模图表上表现出更好的普适性，无需借助人类专业知识或下游验证。我们的代码公开发布在 <a target="_blank" rel="noopener" href="https://github.com/Shen-Lab/GraphCL_Automated%E3%80%82">https://github.com/Shen-Lab/GraphCL_Automated。</a></p>
<h3 id="C²-CRS-Coarse-to-Fine-Contrastive-Learning-for-Conversational-Recommender-System"><a class="header-anchor" href="#C²-CRS-Coarse-to-Fine-Contrastive-Learning-for-Conversational-Recommender-System">¶</a>C²-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System</h3>
<p>Abstract：会话推荐系统（CRS）旨在通过自然语言对话向用户推荐合适的项目。为了开发有效的 CRS，一个主要的技术问题是<font color="red">如何从非常有限的对话上下文中准确推断用户偏好</font>。为了解决这个问题，一个有前途的解决方案是<font color="red">合并外部数据以丰富上下文信息</font>。然而，之前的研究主要集中在为某些特定类型的外部数据设计融合模型，这对于建模和利用多类型外部数据并不通用。<font color="red">为了有效地利用多类型外部数据</font>，我们提出了一种新颖的<font color="red">从粗到精的对比学习框架</font>，以改进 CRS 的数据语义融合。在我们的方法中，我们首先从不同的数据信号中提取和表示多粒度语义单元，然后以由粗到细的方式对齐关联的多类型语义单元。为了实现这个框架，我们设计了用于建模用户偏好的粗粒度和细粒度过程，其中前者侧重于更一般、粗粒度的语义融合，后者侧重于更具体、细粒度的语义融合。这种方法可以扩展到合并更多种类的外部数据。对两个公共 CRS 数据集的大量实验证明了我们的方法在推荐和对话任务中的有效性。细粒度语义融合。这种方法可以扩展到合并更多种类的外部数据。</p>
<p><font color="orange">对比学习、图学习、自监督学习热度比较高</font></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://yangyy.top">yangyy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yangyy.top/posts/3747160458.html">https://yangyy.top/posts/3747160458.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yangyy.top" target="_blank">Yangyy's Life</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/">科研实习</a><a class="post-meta__tags" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2022/10/27/xhVEB6.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/2272983347.html"><img class="prev-cover" src="https://s1.ax1x.com/2022/10/27/xhVEB6.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">我为什么要在校区间往返</div></div></a></div><div class="next-post pull-right"><a href="/posts/1289748485.html"><img class="next-cover" src="https://s1.ax1x.com/2022/10/27/xhVEB6.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">微服务相关知识学习</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/2997172935.html" title="利用RecBole框架复现DCN-V2模型"><img class="cover" src="https://s1.ax1x.com/2022/10/27/xhVEB6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-19</div><div class="title">利用RecBole框架复现DCN-V2模型</div></div></a></div><div><a href="/posts/1979743824.html" title="回顾在RecBole复现模型的这段经历"><img class="cover" src="https://s1.ax1x.com/2022/10/27/xhVEB6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-05</div><div class="title">回顾在RecBole复现模型的这段经历</div></div></a></div><div><a href="/posts/2907034089.html" title="在RecBole当中实现DAGFM模型"><img class="cover" src="https://s1.ax1x.com/2022/10/27/xhVEB6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-13</div><div class="title">在RecBole当中实现DAGFM模型</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/header2.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">yangyy</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">36</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wending0417"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#SIGIR-2022"><span class="toc-number">1.</span> <span class="toc-text">SIGIR 2022</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interpolative-Distillation-for-Unifying-Biased-and-Debiased-Recommendation"><span class="toc-number">1.1.</span> <span class="toc-text">Interpolative Distillation for Unifying Biased and Debiased Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Graph-Trend-Filtering-Networks-for-Recommendation"><span class="toc-number">1.2.</span> <span class="toc-text">Graph Trend Filtering Networks for Recommendation   ***</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HIEN-Hierarchical-Intention-Embedding-Network-for-Click-Through-Rate-Prediction"><span class="toc-number">1.3.</span> <span class="toc-text">HIEN: Hierarchical Intention Embedding Network for Click-Through Rate Prediction  ***</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NAS-CTR-Efficient-Neural-Architecture-Search-for-Click-Through-Rate-Prediction"><span class="toc-number">1.4.</span> <span class="toc-text">NAS-CTR: Efficient Neural Architecture Search for Click-Through Rate Prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Enhancing-CTR-Prediction-with-Context-Aware-Feature-Representation-Learning"><span class="toc-number">1.5.</span> <span class="toc-text">Enhancing CTR Prediction with Context-Aware Feature Representation Learning  ***</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ESCM2-Entire-Space-Counterfactual-Multi-Task-Model-for-Post-Click-Conversion-Rate-Estimation"><span class="toc-number">1.6.</span> <span class="toc-text">ESCM2: Entire Space  Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#On-Device-Next-Item-Recommendation-with-Self-Supervised-Knowledge-Distillation"><span class="toc-number">1.7.</span> <span class="toc-text">On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Single-shot-Embedding-Dimension-Search-in-Recommender-System"><span class="toc-number">1.8.</span> <span class="toc-text">Single-shot Embedding Dimension Search in Recommender System  ***</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Forest-based-Deep-Recommender"><span class="toc-number">1.9.</span> <span class="toc-text">Forest-based Deep Recommender</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Explainable-Fairness-in-Recommendation"><span class="toc-number">1.10.</span> <span class="toc-text">Explainable Fairness in Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#User-controllable-Recommendation-Against-font-color-red-Filter-font-Bubble"><span class="toc-number">1.11.</span> <span class="toc-text">User-controllable Recommendation Against Filter Bubble</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unify-Local-and-Global-Information-for-Top-N-Recommendation"><span class="toc-number">1.12.</span> <span class="toc-text">Unify Local and Global Information for Top-N Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Less-is-More-Reweighting-Important-Spectral-Graph-Features-for-Recommendation"><span class="toc-number">1.13.</span> <span class="toc-text">Less is More: Reweighting Important Spectral Graph Features for Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation"><span class="toc-number">1.14.</span> <span class="toc-text">A Review-aware Graph Contrastive Learning Framework for Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation"><span class="toc-number">1.15.</span> <span class="toc-text">Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AutoLossGen-Automatic-Loss-Function-Generation-for-Recommender-Systems"><span class="toc-number">1.16.</span> <span class="toc-text">AutoLossGen: Automatic Loss Function Generation for Recommender Systems</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number"></span> <span class="toc-text">对比学习专题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SIGIR2022"><span class="toc-number">1.</span> <span class="toc-text">SIGIR2022</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hypergraph-Contrastive-Collaborative-Filtering"><span class="toc-number">1.1.</span> <span class="toc-text">Hypergraph Contrastive Collaborative Filtering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation-v2"><span class="toc-number">1.2.</span> <span class="toc-text">A Review-aware Graph Contrastive Learning Framework for Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation-v2"><span class="toc-number">1.3.</span> <span class="toc-text">Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-level-Cross-view-Contrastive-Learning-for-Knowledge-aware-Recommender-System"><span class="toc-number">1.4.</span> <span class="toc-text">Multi-level Cross-view Contrastive Learning for Knowledge-aware Recommender System</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Knowledge-Graph-Contrastive-Learning-for-Recommendation"><span class="toc-number">1.5.</span> <span class="toc-text">Knowledge Graph Contrastive Learning for Recommendation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KDD2022"><span class="toc-number">2.</span> <span class="toc-text">KDD2022</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CrossCBR-Cross-view-Contrastive-Learning-for-Bundle-Recommendation"><span class="toc-number">2.1.</span> <span class="toc-text">CrossCBR: Cross-view Contrastive Learning for Bundle Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#COSTA-Covariance-Preserving-Feature-Augmentation-for-Graph-Contrastive-Learning"><span class="toc-number">2.2.</span> <span class="toc-text">COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Contrastive-Cross-domain-Recommendation-in-Matching"><span class="toc-number">2.3.</span> <span class="toc-text">Contrastive Cross-domain Recommendation in Matching</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WSDM2022"><span class="toc-number">3.</span> <span class="toc-text">WSDM2022</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Contrastive-Learning-for-Representation-Degeneration-Problem-in-Sequential-Recommendation"><span class="toc-number">3.1.</span> <span class="toc-text">Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Contrastive-Meta-Learning-with-Behavior-Multiplicity-for-Recommendation"><span class="toc-number">3.2.</span> <span class="toc-text">Contrastive Meta Learning with Behavior Multiplicity for Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bringing-Your-Own-View-Graph-Contrastive-Learning-without-Prefabricated-Data-Augmentations"><span class="toc-number">3.3.</span> <span class="toc-text">Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C%C2%B2-CRS-Coarse-to-Fine-Contrastive-Learning-for-Conversational-Recommender-System"><span class="toc-number">3.4.</span> <span class="toc-text">C²-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/1075723047.html" title="ACL Rebuttal &amp; 青岛旅行">ACL Rebuttal &amp; 青岛旅行</a><time datetime="2025-04-07T14:47:09.000Z" title="发表于 2025-04-07 22:47:09">2025-04-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/408765249.html" title="从Windows到Mac，要重新捡起写博客的习惯">从Windows到Mac，要重新捡起写博客的习惯</a><time datetime="2025-03-07T01:23:52.000Z" title="发表于 2025-03-07 09:23:52">2025-03-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/2522645851.html" title="有时候真的不知道该不该遵从自己的本心">有时候真的不知道该不该遵从自己的本心</a><time datetime="2024-04-11T05:00:08.000Z" title="发表于 2024-04-11 13:00:08">2024-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/1030984742.html" title="实验室服务器管理笔记">实验室服务器管理笔记</a><time datetime="2023-12-03T11:13:49.000Z" title="发表于 2023-12-03 19:13:49">2023-12-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/364684684.html" title="我创建了一个南开软件学院信息共享网站">我创建了一个南开软件学院信息共享网站</a><time datetime="2023-07-30T12:46:43.000Z" title="发表于 2023-07-30 20:46:43">2023-07-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025  yangyy</div><div class="footer_custom_text">那些看似不起波澜的日复一日，会突然在某一天让人看到坚持的意义</div><div id="loving-time"></div><script>setInterval(()=>{let create_time=Math.round(new Date(Date.UTC(2014,10,17,17,30,00)).getTime()/1000);let timestamp=Math.round((new Date().getTime()+31*24*3600*1000+8*60*60*1000)/1000);let second=timestamp-create_time;let time=new Array(0,0,0,0,0);if(second>=365*24*3600){time[0]=parseInt(second/(365*24*3600));second%=365*24*3600;}if(second>=24*3600){time[1]=parseInt(second/(24*3600));second%=24*3600;}if(second>=3600){time[2]=parseInt(second/3600);second%=3600;}if(second>=60){time[3]=parseInt(second/60);second%=60;}if(second>0){time[4]=second;}currentTimeHtml='Been spending my life with Zhangyao for '+time[0]+' years '+time[1]+' days '+time[2]+' hours '+time[3]+' minutes '+time[4]+' seconds ♥ ';document.getElementById("loving-time").innerHTML=currentTimeHtml;},1000);	</script><!--<i style="color:#FF6A6A" class="fa fa-heartbeat"></i>--><div id="running-time"></div><script>setInterval(()=>{let create_time=Math.round(new Date(Date.UTC(2022,07,14,11,07,28)).getTime()/1000);let timestamp=Math.round((new Date().getTime()+31*24*3600*1000+8*60*60*1000)/1000);let second=timestamp-create_time;let time=new Array(0,0,0,0,0);if(second>=365*24*3600){time[0]=parseInt(second/(365*24*3600));second%=365*24*3600;}if(second>=24*3600){time[1]=parseInt(second/(24*3600));second%=24*3600;}if(second>=3600){time[2]=parseInt(second/3600);second%=3600;}if(second>=60){time[3]=parseInt(second/60);second%=60;}if(second>0){time[4]=second;}currentTimeHtml='本站已安全运行 '+time[0]+' 年 '+time[1]+' 天 '+time[2]+' 时 '+time[3]+' 分 '+time[4]+' 秒';document.getElementById("running-time").innerHTML=currentTimeHtml;},1000);	</script><img src="https://www.yiqiuwl.com/content/uploadfile/202208/cffa1659599066.png">
<a href="http://beian.miit.gov.cn/"  style="color:#f72b07" target="_blank">闽ICP备2022015544号</a></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'Srx46WYtgI2COUqAi761ccR8-gzGzoHsz',
      appKey: 'bNpdyeUlKIvb6bUb42mYvy0h',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>